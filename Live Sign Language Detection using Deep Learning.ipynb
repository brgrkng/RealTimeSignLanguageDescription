{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276ed44d",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "A large portion of the world's citizens are unable to speak due to birth defects or other unfortunate events. Sign language was developed to circumvent the problems that arise from this and allow non-speaking people to communicate just as anyone else would. Unfortunately, only an even smaller segment of the world's populace are able to understand sign language. In this paper, we used computer vision and machine learning techniques to create a model that is able to detect and correctly classify various sign language gestures. We reviewed 2 research articles that have previously tackled the task of sign language detection and studied their approach to the problem. Taking inspiration from their methods, we created our own sign language detection model using deep learning with a Single Shot Detection (SSD) neural network. We achieved a model that is capable of identifying sign language symbols within its dataset at an average speed of 700ms and an accuracy of 100%. We have proven the creation of such a model is possible and at a larger scale would greatly improve the lives of non-speakers, as well society as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513604ed",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Intelligence among living creatures is not rare. What then, has allowed human beings to take over the world the way we have? Some say it's simply superior intelligence, others say that our ability to create and make use of tools has allowed us to accomplish any goal. These are important factors that has allowed humans to thrive, but at the core of it all is our ability to take advantage of the strengths of every individual. To share, pass down information through generations, create organized civilations and governments - we can do all of this because we have the ability to share complex ideas and thoughts through speech. \n",
    "\n",
    "However, unfortunate events or birth defects cause some of us to be unable or struggle to hear and/or speak. For this reason, sign language was created to allow people with said defects to be able to communicate fluently with the same complexity we have through speech. However by it's nature, sign language - much like regular spoken language - is difficult to learn. It is a daunting task to learn any new language and sign language is no different, and may in many cases be more difficult.\n",
    "\n",
    "The paper \"Handling Sign Language Data: The Impact of Modality\" by Quer and Steinbach (2019) states that less than 5-10% of individuals with hearing or speaking disabilities are born to parents with similar conditions and hence are unlikely to receive the proper education required and as a result, less than half of speaking and hearing impaired people are fully capable of communicating, through sign language or otherwise. With over 1.5% of the world's population being born with communication related disabilities and more than 20% of the world being at risk of hearing loss, less than 0.5% are able to communicate fluently through sign language.\n",
    "\n",
    "Through the use of modern machine learning techniques, we are able to develop computer vision models which are able to detect different patterns in data. These pattern detection methods could be applied to understand and translate sign language to natural language. This paper will focus on and study and demonstrate modern day applications of machine learning to attempt to understand sign language, through the use of computer vision.  \n",
    "\n",
    "## Research Goal\n",
    "\n",
    "We understand that there are problems with modern implementations of sign language. Most of the population do not understand because it is difficult to learn, and most people are not given a fair opportunity to do so. Modern technology allows the combination of computer vision and machine learning classification techniques to automate the task of reading hand signs, labeling their meaning and making it visible to the user. We attempt to understand the process behind the creation of such models and how they can be applied to solve the problem posed by the lack of sign language understanding amongst the population.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this paper, we review two other research articles attempting to apply machine learning to match real examples of sign language use, to the training set of pre-labeled images of hand signs used in the specified language. We can adapt these different solutions and demonstrate how we ourselves can create a model that can use a live video feed from a webcamera to be able to classify sign language symbols in real time and assign a confidence level to each symbol.\n",
    "\n",
    "# Related Work\n",
    "\n",
    "## Overview\n",
    "\n",
    "The papers below discuss different methods to collect and pre-process data, and use the data to train a deep learning model correctly classify static sign language symbols to their appropriate label. These papers were chosen specifically because of their similarity to the chosen topic and the use of recent technological advancements. Both papers discuss the use of a convolutional neural network (CNN) and its construction is described in detail.\n",
    "\n",
    "## Paper 1 : \"Static Sign Language Recognition Using Deep Learning\" by Juan et. al. - 2019\n",
    "\n",
    "The undertaking describes the creation of a machine learning model based on CNNs to create a learning instrument for students seeking to understand American Sign Language (ASL). It is based on differentiating skin textures from non-skin texures in order to detect the signs being formed. The skin tone range was manually predefined by the researchers. Images containing sign language symbols being formed were fed as input to the CNN created using the popular Python library known as Keras. With appropriate illumination, picture quality and background, the model was able to achieve accuracies ranging from 90% to 97.5% for different parts of speech in ASL. The model was built with the goal of achieving fast compute times, allowing for real time sign language recognition - this requirement was satisfied with an average recognition time of 3.93 seconds. \n",
    "\n",
    "\n",
    "## Paper 2 : \"Deep learning-based sign language recognition system for static signs\" by Wadhawan and Kumar - 2019\n",
    "\n",
    "The paper discusses another model based on the CNN architecture with the goal of recognizing Indian Sign Language (ISL). The model was trained using 35,000 images of 100 different ISL hand signs that were collected from a variety of users. The images are resized to the same desired resolution and used as input to the CNN. 50 different CNNs were built and the best results were shown where the number of layers were decreased - a higher accuracy was seen in a model with 4 layers as compared to a model with 8 layers. Different optimizers were also tested with each of the 50 CNNs and it was seen that a model utilizing 4 layers, 16 filters and the Adam optimizer achieved the best results, with 99.17% accuracy on colored images and 99.80% accuracy on grayscale images.  \n",
    "\n",
    "\n",
    "## Critical Review\n",
    "\n",
    "The neural network architecture described by Wadhawan and Kumar results in highly accurate models capable of correctly labeling a 100 different types of signs. The drawback to this approach is the required dataset, hardware and training time. The hardware described in the paper consists of industrial data center grade graphics cards and memory units that are not economically viable for the scope of this demonstration. On the other hand, Juan et. al describes a model that can process signs in real time. A side effect of this property is that it allows the model to be lightweight in nature and can be trained on less powerful hardware. The lower accuracy of 90%, compared to the 99% of Wadhawan and Kumar's paper is an acceptable difference for the current research objective. Furthermore, it is not clear if fast recognition of sign language is possible using the Wadhawan-Kumar model at all, regardless of the hardware being used and hence the Juan et al. paper's architecture is more appropriate for this application.\n",
    "\n",
    "An important concept discussed in the Juan et al. paper is the focus on differentiating skin-tone colors from non-skin-tone colors. This approach is flawed because an image cannot perfectly represent a skin texture. Over or underexposure to light, unseen skin-tones and variation of skin textures may harm the model's detection of symbols being made. The only situation where this approach is superior to others is when there is a background that may be too close in color to the specific skin-tone. However, this situation can be understood to be a much rarer occurence than varying light conditions. \n",
    "\n",
    "## Takeaways and adaptations to this paper\n",
    "\n",
    "Convolutional neural networks are strong tools used for computer vision based machine learning projects. They are able to take an image as an input and use primitive techniques to uniquely identify different features of the image, hence performing feature extraction directly from the data without having to manually define these features ourselves. The papers above leverage this strength to create highly accurate models to detect sign language. We will also be using the CNN architecture, however we will be making use of more modern neural network techniques, available to us through the Tensorflow python libraries. \n",
    "\n",
    "The Juan et. al. paper describes a fast CNN architecture that gives us results in an average of 3.93s, however our paper is more focused towards live detection and hence we need an architecture that is even faster. To accomplish this, we will be making use of an Single Shot Detector (SSD) network. SSDs are a modern approach to real time object detection and are well suited to the objective. \n",
    "\n",
    "SSDs are based on convolutional networks and are able to extract features from images in much the same way. It takes as input a regular frame from what would be a real time feed and ground truth - acting as labels to be identified by the detector network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524554ff",
   "metadata": {},
   "source": [
    "# Methodology and Implementation\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "A machine learning model capable of detecting and classifying sign language has various applications. As described in the Juan et. al. paper, it can be used as a learning tool for people seeking to study sign language. Traditionally, this learning is done with a peer or teacher who is already well versed. An immediete problem with human teaching is the feedback process. The teacher will need to communicate verbally (something which may not be possible with deaf students) and repeatedly instruct the students on better techniques. This is a tedious and time consuming process. With an automated system, the user will be able to practice fluidly with instant visual feedback - allowing them to quickly determine whether or not their execution is adequate. \n",
    "\n",
    "A further use of the model can be seen in live translations. People who do not understand sign language would be able to do so with the use of the model as an interpreter. The process would be as simple as pointing a camera to the sign language user and each symbol would be identified and displayed to the user in real time. Communication between sign language users and non-sign language users would be made possible and simple.\n",
    "\n",
    "The use of a machine learning model makes this all possible without the use of an internet connection. The detection can be used in an offline environment, only requiring a system with a camera and enough processing power to run the model. This requirement is satisfied by a majority of modern devices. \n",
    "\n",
    "## Analytical Approach\n",
    "\n",
    "We require a model that is capable of detecting hands making specific gestures, identify and label the specific gesture being made and assign a confidence level to each detection. This must all be done in real time. A machine learning approach that is known to satisfy these criteria is a Single Shot Detector neural network. SSD networks are an extention of CNN architecures, able to extract features in the same way. The focus of this network lies in real time object detection. Labeled objects within larger images are used for the training data and the NN attempts to match features identified within the labeled space to features in the new input data. The strength of the SSD architecture is its speed and capability to run multiple classifications at once or in quick succession. \n",
    "\n",
    "Pre-existing object detection models using SSD architectures are available in the public Tensorflow research repository on Github. We can use transfer learning techniques to repurpose these models to be specific to detect hand signs defined by our own dataset.\n",
    "\n",
    "An SSD network satisfies the business requirements and the availability and hence the ease of creating such a model allows us to proceed with model development using the SSD architecture for our neural network. \n",
    "\n",
    "\n",
    "## Data Requirements\n",
    "\n",
    "The data requirements depend on our objective and chosen model. \n",
    "\n",
    "We require images of sufficient quality, depicting the use of sign language as static hand symbols. The SSD network requires this data to be labeled. Hence, a data file will be required alongside each image, containing information about the label being depicted in the image, and the coordinates of the object (hand sign) to be detected and classified. \n",
    "\n",
    "The coordinates will be required to narrow down the region in which features must be extracted in order to be matched to future inputs. The whole image is still required - this is to allow \"context\" for each image. \n",
    "\n",
    "The whole dataset needs to include a sufficient amount of pictures for each symbol. This includes both training and testing data. Variety in the data is required -  camera angles, distances from the camera and lighting conditions must be varied in order to allow the model to be able to identify the same signs being made with different surroundings and in different sizes. \n",
    "\n",
    "For this project, we have chosen to select 6 different static sign language words for testing, they are : Hello, Yes, No, Goodbye, Thank You, I Love You. 15 images are collected for each sign, with 12 being set aside for training and 3 used for testing - giving us a 80:20 train:test ratio. \n",
    "\n",
    "## Data Collection\n",
    "\n",
    "The data was collected using a 720p webcam. As described in the requirements, 15 pictures of each of the 6 static sign language symbols were performed infront of the camera and were captured, giving the dataset a total size of 90 images. The requirements of the contents of the image were followed closely, making sure to each image depicting the same symbol were captured in different positions, some being slightly similar while others were drastically different. An example is shown below.\n",
    "\n",
    "####INSERT FIGURE 1 HERE\n",
    "\n",
    "Figure 1 depicts three different images used to show the symbol \"Goodbye\". The first image shows a high quality image with a typical use of the symbol. The second image shows the same symbol being made with the opposite hand. The third image shows a lower quality image with a varied hand position. These variations in image quality, and body/hand position are repeated throughout the dataset for the purpose of data variety. \n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "The key feature of each image is of course the class of the hand sign being depicted. Other important features are the dimensions (and hence the quality) of the image, the relative amount of light present in each picture, the position of the sign within the picture and the relative size of the symbol in the picture (relates to distance from the camera, the sign being large indicates it being closer to the camera). \n",
    "\n",
    "These features have been varied throughout the different images. This is because in a real time video feed, there will be constant changes in the environment and lighting, along with signs being seen from various angles and distances from the camera. The similarities in the features of the hand symbol despite the differences of the mentioned features is the problem that will be tackled by the SSD network. These features are not explicitly defined or annotated by us, but are extracted through the neural network, directly from the image. The only exception being the hand sign itself - we manually annotate and label the region containing the hand sign.\n",
    "\n",
    "## Data Preperation\n",
    "\n",
    "The first stage in data preperation is to appropriately annotate and label the images as necessary. For this process, we use a python program known as \"label image\". The program allows us to annotate specific areas of an image, and save this data into a seperate file.\n",
    "\n",
    "####INSERT FIGURE 2 HERE\n",
    "\n",
    "####INSERT FIGURE 3 HERE\n",
    "\n",
    "The figures above show an example of how one image would be prepared. The image is taken and the section containing the hand sign is annotated with the name of the label and the coordinates of the boundaries of the sections. The data is saved in the format of an XML file as shown in Figure 3. \n",
    "\n",
    "The data is then seperated into training and test data. We use 80% for training and 20% for testing. This gives us 12 images for training and 3 for testing for each sign, giving us a total of 72 images for our training set and 18 images for our test set. \n",
    "\n",
    "For processing by the model, a label map is required. The label map assigns each label a specific id. This is created with the follow code.\n",
    "\n",
    "####INSERT LABELMAP CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a91f8",
   "metadata": {},
   "source": [
    "This gives us the follow file. \n",
    "\n",
    "####INSERT LABEL MAP\n",
    "\n",
    "Another requirement by the model is known as a Tensorflow record. The following script is provided by the Tensorflow library to create these records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVED LOCALLY AS generate-tfrecord.py\n",
    "\n",
    "\"\"\" Sample TensorFlow XML-to-TFRecord converter\n",
    "\n",
    "usage: generate_tfrecord.py [-h] [-x XML_DIR] [-l LABELS_PATH] [-o OUTPUT_PATH] [-i IMAGE_DIR] [-c CSV_PATH]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -x XML_DIR, --xml_dir XML_DIR\n",
    "                        Path to the folder where the input .xml files are stored.\n",
    "  -l LABELS_PATH, --labels_path LABELS_PATH\n",
    "                        Path to the labels (.pbtxt) file.\n",
    "  -o OUTPUT_PATH, --output_path OUTPUT_PATH\n",
    "                        Path of output TFRecord (.record) file.\n",
    "  -i IMAGE_DIR, --image_dir IMAGE_DIR\n",
    "                        Path to the folder where the input image files are stored. Defaults to the same directory as XML_DIR.\n",
    "  -c CSV_PATH, --csv_path CSV_PATH\n",
    "                        Path of output .csv file. If none provided, then no file will be written.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import io\n",
    "import xml.etree.ElementTree as ET\n",
    "import argparse\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import tensorflow.compat.v1 as tf\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util, label_map_util\n",
    "from collections import namedtuple\n",
    "\n",
    "# Initiate argument parser\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Sample TensorFlow XML-to-TFRecord converter\")\n",
    "parser.add_argument(\"-x\",\n",
    "                    \"--xml_dir\",\n",
    "                    help=\"Path to the folder where the input .xml files are stored.\",\n",
    "                    type=str)\n",
    "parser.add_argument(\"-l\",\n",
    "                    \"--labels_path\",\n",
    "                    help=\"Path to the labels (.pbtxt) file.\", type=str)\n",
    "parser.add_argument(\"-o\",\n",
    "                    \"--output_path\",\n",
    "                    help=\"Path of output TFRecord (.record) file.\", type=str)\n",
    "parser.add_argument(\"-i\",\n",
    "                    \"--image_dir\",\n",
    "                    help=\"Path to the folder where the input image files are stored. \"\n",
    "                         \"Defaults to the same directory as XML_DIR.\",\n",
    "                    type=str, default=None)\n",
    "parser.add_argument(\"-c\",\n",
    "                    \"--csv_path\",\n",
    "                    help=\"Path of output .csv file. If none provided, then no file will be \"\n",
    "                         \"written.\",\n",
    "                    type=str, default=None)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.image_dir is None:\n",
    "    args.image_dir = args.xml_dir\n",
    "\n",
    "#label_map = label_map_util.load_labelmap(args.labels_path)\n",
    "#label_map_dict = label_map_util.get_label_map_dict(label_map)\n",
    "\n",
    "label_map_dict = label_map_util.get_label_map_dict(args.labels_path)\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    \"\"\"Iterates through all .xml files (generated by labelImg) in a given directory and combines\n",
    "    them in a single Pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    path : str\n",
    "        The path containing the .xml files\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "        The produced dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height',\n",
    "                   'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "\n",
    "def class_text_to_int(row_label):\n",
    "    return label_map_dict[row_label]\n",
    "\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(args.output_path)\n",
    "    path = os.path.join(args.image_dir)\n",
    "    examples = xml_to_csv(args.xml_dir)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "    print('Successfully created the TFRecord file: {}'.format(args.output_path))\n",
    "    if args.csv_path is not None:\n",
    "        examples.to_csv(args.csv_path, index=None)\n",
    "        print('Successfully created the CSV file: {}'.format(args.csv_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe7d1a",
   "metadata": {},
   "source": [
    "We create Tensorflow records for the training and testing data with the following commands executed on the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0297f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {'Tensorflow/scripts' + '/generate_tfrecord.py'} -x {'Tensorflow/workspace/images/train'} -l {'Tensorflow/workspace/annotations/label_map.pbtxt'} -o {'Tensorflow/workspace/annotations/train.record'}\n",
    "!python {'Tensorflow/scripts' + '/generate_tfrecord.py'} -x{'Tensorflow/workspace/images/test'} -l {'Tensorflow/workspace/annotations/label_map.pbtxt'} -o {'Tensorflow/workspace/annotations/test.record'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82483d97",
   "metadata": {},
   "source": [
    "All required data is prepared and we proceed to building our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb0a98",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We will be using an SSD network, along with transfer learning techniques in order to apply pre-trained object detection models to help train our sign language detection model. \n",
    "\n",
    "###INSERT FIGURE 4 HERE - SSD NETWORK DIAGERAM \n",
    "\n",
    "Figure 4 shows the layer architecture of the SSD network used by the Tensorflow library for object detection. It consists of a pooling layer, and several convolutional layers. The depth of these convolutional layers depend on the number of classes the model is being trained for.\n",
    "\n",
    "Different aspects of the model are configured using the configuration file. Each relevant section of the config is explained in detail below. \n",
    "\n",
    "####INSERT 5 1\n",
    "\n",
    "This shows the base configuration of the SSD network. We set the number of classes to be identified to be 6. Each image is resized to the dimensions 320x320 (height x width).\n",
    "\n",
    "####INSERT 5 2\n",
    "\n",
    "These are the specified configurations of the section of the neural network used to extract features from each image. Here we define the type of feature extractor and minimum depth. We also define the parameters of the convolutional network - the type of regulizer used, the initializer and the activation type. The specific settings are shown in the figure.\n",
    "\n",
    "####INSERT 5 3\n",
    "\n",
    "We set the configurations for the matcher. We set the minimum confidence level for a match being seen to be 50% \n",
    "\n",
    "####INSERT 5 4\n",
    "\n",
    "Much like in Figure 5.2, we set the specific parameters of the box predictor. Once again we use a regulizer, intializer and activation.\n",
    "\n",
    "####INSERT 5 5\n",
    "\n",
    "Here we set the configuration for the required training steps. We define the batch size, which is the number of samples we process before making changes to the model. A higher number will require more computing and vice versa. As the size of our dataset is relatively low, we set this to be 4. \n",
    "\n",
    "Data augmentation settings are defined to assume how much the the data should be allowed to change. We set that an object's features being flipped horizontally should still be detected, furthermore we define the different aspect ratios and size of the object relative to it's original size. The chosen optimizer is a momentum optimizer and configured with a specified base learning rate and learning rate decay. \n",
    "\n",
    "We set the options for fine tuning to be of the detection type and use a pretrained model checkpoint to fine tune detections made appropriately.\n",
    "\n",
    "####INSERT 5 6\n",
    "\n",
    "Here we define the input to be read. We feed in our previously defined label map and tensorflow record file created for our training data. \n",
    "\n",
    "We also setup the evaluation metrics to be shown during training and similarly to our training data, feed in our label map and tensorflow record for our test data.\n",
    "\n",
    "The configurations for the model are prepared and we can begin training the model. This is done with the following command.\n",
    "\n",
    "####INSERT 6\n",
    "\n",
    "####INSERT 7 1\n",
    "\n",
    "We can see that the model has begun training with the initial loss metrics being shown\n",
    "\n",
    "####INSERT 7 1\n",
    "\n",
    "The model has concluded training and the final loss metrics are displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea16a4",
   "metadata": {},
   "source": [
    "## Results and Evaluation\n",
    "\n",
    "The model is used to predict the classes seperated into the test data. The following results were observed in the following confusion matrix.\n",
    "\n",
    "####INSERT CM\n",
    "\n",
    "Common metrics for computer vision based machine learning models are the average precision and recall. These are calculated as follows.\n",
    "\n",
    "####INSERT PRECISION\n",
    "\n",
    "Where, TP = True Positive, FP = False Positive, FN = False Negatives.\n",
    "\n",
    "Therefore, we calculate \n",
    "\n",
    "Average Precision = 1\n",
    "Recall = 1\n",
    "\n",
    "The model has correctly classified each of the sign language classes depicted in the images and hence the evaluation metrics give us a perfect precision and recall score of 1. We can conclude that the model is able to accurately predict each sign language class.\n",
    "\n",
    "\n",
    "## Deployment\n",
    "\n",
    "The model is deployed with a real time feed from a webcam with the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2575c8ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m image_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(frame)\n\u001b[0;32m     37\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(np\u001b[38;5;241m.\u001b[39mexpand_dims(image_np, \u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 38\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m num_detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(detections\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_detections\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     41\u001b[0m detections \u001b[38;5;241m=\u001b[39m {key: value[\u001b[38;5;241m0\u001b[39m, :num_detections]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     42\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m detections\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file('RealTimeObjectDetection/Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config')\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join('RealTimeObjectDetection/Tensorflow/workspace/models/my_ssd_mobnet', 'ckpt-11')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap('RealTimeObjectDetection/Tensorflow/workspace/annotations/label_map.pbtxt')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=5,\n",
    "                min_score_thresh=.5,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (1920, 1080)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dde7b",
   "metadata": {},
   "source": [
    "This launches a window with a webcam feed, annotating the sign language symbols as they are detected on screen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fb26c",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## \"Static Sign Language Recognition Using Deep Learning\" by Juan et. al. - 2019\n",
    "A strength of the model seen in the Jaun et. al. paper is the model's speed. It is able to achieve an adequately high accuracy. An element of the design that can be considered both a flaw and a strength, is seen in the design's explicit focus on static backgrounds, lighting and predefined skin-tone ranges. The results show an average of 93.67% accuracy in its predictions but this is only within the ideal conditions set by the researchers. The model may perform poorly outside of these conditions. The predefined skin-tone may not be able to properly detect hands with skin tones outside or at the edges of the thresholds and furthermore it may be influenced by the background. A background with a color matching the skin tone of the user may reveal problems with the detection model. To conclude, the model works well in a controlled environment, but with the intention of being used as a learning tool, more often than not, the background, lighting, camera angle and the user's skin tone will not be perfectly desirable to the model. \n",
    "\n",
    "## \"Deep learning-based sign language recognition system for static signs\" by Wadhawan and Kumar - 2019\n",
    "In contrast, the model observed in the Wadhawan and Kumar paper sacrifices speed in preference to accuracy. 50 different models were constructed and the best model was chosen strictly on its ability to correctly identify the label regardless of the computation time, achieving an accuracy of 99.9% on greyscale images. Such a high evaluation requires us to investigate the possibility of overfitting. The model has been created with a dataset including 35,000 images of 100 different static signs. A dataset with a high number of records does not directly imply overfitting has occurred, but it this case this judgement is appropriate because with the addition of more and more images, the dataset does become more complex with an increasing amount of features and scenarios. Thus we can say that overfitting has occured and the model's evaluation is inflated. Although the model's evaluation may not be accurate, a strength of such a complex dataset is that it is able to capture a large variety of situations. There are few situations which are unseen and hence in real world application would run better than a model with a smaller dataset. \n",
    "\n",
    "## Our implementation\n",
    "\n",
    "We end up with a model with a 100% accuracy, according to evaluation against our test data. This is of course, too good to be true. In our demonstration, we notice certain false positives with the model detecting certain sections of the background as one of the sign language symbols, which is of course incorrect. This can be attributed to multiple factors regarding our dataset. The size of the dataset only consists of 6 different sign language symbols with 15 images depicting each label. Comparing to the datasets used in the Juan et. al and Wadhawan and Kumar papers, we know that this dataset is very small. The model has few examples to learn from and match to. Hence overfitting is the likely culprit of the high accuracy.\n",
    "\n",
    "This problem is further made clear in the model deployment. We attempted to change the lighting in the room to see if the model would still be able to detect the signs being made and it was seen that the model struggled to detect any signs at all with only occasional detections which were not at all accurate. This tells us that the dataset used was inadequete in both size and variety.\n",
    "\n",
    "####COMPARE PICTURES\n",
    "\n",
    "There is a noticable delay in the hand signs being formed on the video and being detected and classified in the box predictors. The average delay recorded was about 700ms. This is a noticable delay but nearly 6 times faster than the CNN model discussed in the Juan et. al paper. 700ms is acceptable for individual signs, however multiple signs stringed together may cause a problem as some signs would not have enough time to be properly detected and classified. \n",
    "\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "An immediete remedy to our implementation's overfitting and inability to detect labels under different circumstances would be with an increased dataset size. We have confirmed that the model is able to detect the same hand sign in different positions and camera angles. However, our dataset did not account for different lightings or backgrounds. A dataset could be constructed with larger considerations for variation and with a larger amount of entries, the model would be able to overcome these imperfections.\n",
    "\n",
    "Another workaround to this problem was seen in the Jaun et. al., where certain features of the subject are not left to the model's feature extractor and instead manually set as a predefined parameter. This however would take much more manual labor and potentially lead to less than favorable results as compared to increasing the quality and size of our dataset.\n",
    "\n",
    "Our model detects signs faster than the above papers and yet the delay is still noticeable. SSDs are fast detection networks but there are comparable neural network architectures that rival and in certain cases surpass SSDs. These are known as Faster Regional CNN models. They are capable of combining different regions into similar regions and running fast detection algorithms that are also popular for object detection models. The alternate neural network architecture of Faster R-CNN models would be a suitable alternative to the SSD network used in this implementation.\n",
    "\n",
    "The model detects sign language gestures individually but does not concern itself with gestures made previously as context to predict the current or future symbols. This is a problem that is tackled by language models, using probability distributions to calculate the probability of a certain word given a number of previous words. This can be implemented into our system using a Hidden Markov Model (HMM) using Bayes' Theorem to calculate the probability of the current symbol being detected, along with the base detection algorithm. This is especially useful when the user is attempting to form sentences using sign language gestures in fluid succession. With this additional context, the model may be more accurate in its classifications. \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Communication is a core building block of society and the progress of humanity and unfortunately a large portion of the population struggle to participate due to birth defects or other unfortunate circumstances. Sign language provides individuals unable to speak to still be able to conveniently communicate just as effectively as anyone else. Just like any other spoken language however, it must be learned and practiced for years on end. Furthermore, even if one were to take the time to learn sign language to an effective level, it would not be usable in regular life unless everyone they were trying to communicate to also understood this language. Unfortunately, not everyone has the time for such an undertaking.\n",
    "\n",
    "In regular languages, machine learning techniques have been used with natural language processing to understand the meaning of a body of text in one language and hence form a sentence with the same meaning in another language. This form of translation is not possible with sign language as we have no text to work with, however, we do have computer vision. We have created a model which is able to look at any sign language gestures within the dataset and instantaneously display its meaning on screen.\n",
    "\n",
    "This can be used by both speaking and non-speaking people to use as a tool to ease the process of learning sign language. Furthermore, it can be used as a direct translation tool, foregoing the learning process entirely only requiring a video input from any device. \n",
    "\n",
    "The creation of such a model has greater implications for the progress of society. The progress and impact of a country on the world can largely depend on its population. The advent of an instant translation tool for such a large portion of the world's population is equivalent to unlocking huge untapped potential. Many non-speaking people struggle to find jobs or participate in activities that increase the productivity and strength of any given field and this tool may allow them to do so.\n",
    "\n",
    "# Main References\n",
    "\n",
    "Tolentino, L., Juan, R., Thio-ac, A., Pamahoy, M., Forteza, J., & Garcia, X. (2019). Static Sign Language Recognition Using Deep Learning. International Journal of Machine Learning and Computing. Retrieved 9 October 2022, from https://www.researchgate.net/publication/337285019_Static_Sign_Language_Recognition_Using_Deep_Learning.\n",
    "    \n",
    "Wadhawan, A., & Kumar, P. (2019). Deep learning-based sign language recognition system for static signs. Retrieved 9 October 2022, from https://doi.org/10.1007/s00521-019-04691-y.\n",
    "    \n",
    "\n",
    "# Other References\n",
    "\n",
    "Shapiro, L. (2018). Computer vision: the last 50 years. Taylor & Francis. Retrieved 9 October 2022, from https://doi.org/10.1080/17445760.2018.1469018.\n",
    "\n",
    "Quer, J., & Steinbach, M. (2019). Handling Sign Language Data: The Impact of Modality. Retrieved 9 October 2022, from https://pubmed.ncbi.nlm.nih.gov/30914998/.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
